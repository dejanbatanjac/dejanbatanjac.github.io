---
published: true
layout: post
title: Variational Auto-Encoder
permalink: /vae
---


There are two approaches to *inference*:
* exact inference
* approximation inference

<!-- **Exact_ inference** algorithms would be:

* Brute force
* The elimination algorithm
* Message passing
* Junction tree algorithm -->
<!-- 
Other approximation inference algorithms wold be:
* Loopy belief propagation
* Variational inference
* Stochastic inferencesimulation  -->

Variational Auto-Encoders do **variational Inference** thus the name and this is an **approximation** inference type.

## Marginal Likelihood

To perform **inference** it is sufficient to reason in terms of probabilistic model **marginal likelihood** which is marginalization of **any missing** or latent variables in the model. As we will see these will be our $\boldsymbol{z}$ variables.

This integration is typically **intractable**, and instead, we optimize a lower bound on the marginal likelihood.

VI algorithm is to compute the posterior probability **approximately** using the **neural net**.


## Probabilistic machine learning

A probabilistic model is a joint distribution of hidden variables $\boldsymbol {z}$ and observed variables $\boldsymbol {x}$:

$$
p(\boldsymbol {z}, \boldsymbol {x})
$$

Inference about the unknowns is through the posterior, the conditional distribution of the hidden variables given the observations:

$$
p(\boldsymbol {z} \mid \boldsymbol {x})=\frac{p(\boldsymbol {z}, \boldsymbol {x})}{p(\boldsymbol {x})}
$$

**Question**: What is the evidence here?

It's $p(\boldsymbol x)$, and we should learn this distribution so we can create our own inputs from this distribution.


For the most interesting models the denominator $p(\boldsymbol x)$ is untractable. We can only approximate posterior inference.

Variational Auto-Encoder assumes:

* input $\boldsymbol x$
* latent space $\boldsymbol  z$
* VAE NN parameters $\boldsymbol w$
* PDF of joint $p_{\boldsymbol w}(\boldsymbol  x, \boldsymbol  z)$  is differentiable

The whole idea of VAE is to create a neural net that approximates the **posterior**.

Posterior distribution $p(\boldsymbol z \mid \boldsymbol  x)$ is distribution of latent variable condition the data. This distribution is **continuous**.

Since we will learn the VAE as neural net, all the neural net parameters are weights so we can rewrite the distribution with respect to the weights: $p_{\boldsymbol w}(\boldsymbol z \mid \boldsymbol x)$.



<!-- 
## Maximum likelihood

Maximum likelihood is a method for estimating parameters by maximizing the probability of the observed data. The main ingredients are:

 - The data: $D=(X,Y)$
 - The model parameters: $\theta$
 - The model that relates data to the parameters: $P(D|\theta)$ (which can be written differently, depending on the situation: $P(Y|X, \theta)$ is one of the possibilities.)

One would typically name the probability of observing data given parameters *likelihood* and write it as $L(\theta|D)=P(D|\theta)$. This is just a change of notation. One then maximizes the likelihood in respect to the values of the parameters:
\begin{equation}
\hat{\theta}=\mathrm{argmax}_\theta L(\theta|D).
\end{equation}

Quite often the likelihood is a product of many identical functions for different data points. In this case it is mathematically more convenient to maximize its logarithm, i.e. the *log-likelihood*: $LL(\theta|D) = \log L(\theta|D)$. The likelihood and its logarithm has the same maximum, since logarithm is a monotonous function. Finally, maximizing a function is the same as minimizing its negative, therefore minimizing a *negative log-likelihood* is the same as maximizing the *log-likelihood*, which is the same as maximizing the likelihood. This is important since the optimization algorithms are often spelled explicitly only for minimization (or only for maximization).

When d is dimension of multivariate normal then the PDF is given as:

p(x) = $(2 \pi)^{-\frac{d}{2}} \operatorname{det}(\boldsymbol{\Sigma})^{-\frac{1}{2}} e^{-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^{\top} \boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})}$

Where $\Sigma$ is positive semi definite.




A maximum likelihood estimator coincides with the most probable Bayesian estimator given a uniform prior distribution on the parameters. Indeed, the maximum a posteriori estimate is the parameter $\theta$ that maximizes the probability of $\theta$ given the data, given by
Bayes' theorem:
$$
\mathbb{P}\left(\theta \mid x_{1}, x_{2}, \ldots, x_{n}\right)=\frac{f\left(x_{1}, x_{2}, \ldots, x_{n} \mid \theta\right) \mathbb{P}(\theta)}{\mathbb{P}\left(x_{1}, x_{2}, \ldots, x_{n}\right)}
$$
where $\mathbb{P}(\theta)$ is the prior distribution for the parameter $\theta$ and where $\mathbb{P}\left(x_{1}, x_{2}, \ldots, x_{n}\right)$ is the probability of the data averaged over all parameters. Since the denominator is independent of $\theta,$ the Bayesian estimator is obtained by maximizing $f\left(x_{1}, x_{2}, \ldots, x_{n} \mid \theta\right) \mathbb{P}(\theta)$ with respect to $\theta$. If we further assume that the prior $\mathbb{P}(\theta)$ is a uniform distribution, the Bayesian estimator is obtained by maximizing the likelihood function $f\left(x_{1}, x_{2}, \ldots, x_{n} \mid \theta\right)$. Thus the Bayesian estimator coincides with the maximum likelihood estimator for a uniform prior distribution $\mathbb{P}(\theta)$.

https://math.stackexchange.com/questions/892832/why-we-consider-log-likelihood-instead-of-likelihood-in-gaussian-distribution/892874#892874


It means expectation with respect to $q_{\phi}\left(\mathbf{z} \mid \mathbf{x}^{(i)}\right) .$ So:
$$
\mathbb{E}_{q_{\phi}\left(\mathbf{z} \mid \mathbf{x}^{(i)}\right)}\left[\log p_{\theta}\left(\mathbf{x}^{(i)} \mid \mathbf{z}\right)\right]=\int_{\mathbb{R}^{d}} q_{\phi}\left(\mathbf{z} \mid \mathbf{x}^{(i)}\right) \log p_{\theta}\left(\mathbf{x}^{(i)} \mid \mathbf{z}\right) d \mathbf{z}
$$
Where without further information on the dimensionality of $\mathbf{z}$ I have assumed it to be in $\mathbb{R}^{d}$.
To further clarify, note that the underlying random vector/source of randomness is $\mathbf{z}$, of which you are computing the expectation of a function $f(\mathbf{z}),$ where $f(\mathbf{z})=\log p_{\theta}\left(\mathbf{x}^{(i)} \mid \mathbf{z}\right) .$ And this underlying source of randomness is captured in the distribution $q_{\phi}\left(\mathbf{z} \mid \mathbf{x}^{(i)}\right)$.


$\begin{aligned} \mathbb{E}_{q\left(\mathbf{z} \mid \mathbf{x}^{(i)} ; \phi\right)}\left[\log p_{\theta}\left(\mathbf{x}^{(i)} \mid \mathbf{z}\right)\right] &=\int_{\mathbb{R}^{d}} q\left(\mathbf{z} \mid \mathbf{x}^{(i)} ; \phi\right) \log p\left(\mathbf{x}^{(i)} \mid \mathbf{z} ; \theta\right) d \mathbf{z} \\ &=h(\phi, \theta) \end{aligned}$ -->

## How VAE works

We start from the **input** $\boldsymbol x \in \mathbb R^n$, and we **encode** it not just with $\mathbb E(\boldsymbol{z})$ like in normal autoencoder but we also keep the variance $\mathbb V(\boldsymbol{z})$. Both $\mathbb E(\boldsymbol{z})$ and $\mathbb V(\boldsymbol{z})$ have dimensionality $\mathbb R^d$ so in total after the encoding step we have $\mathbb R^{2d}$.

**Encoder** : $\mathbb R^n \rightarrow \mathbb R^{2d}$

**Decoder** : $\mathbb R^d \rightarrow \mathbb R^{n}$ 


![vae steps](/images/2021/04/vae-steps.png) 

The **sampler** is capable to create samples from the Gaussian distribution with $\mathbb E(\boldsymbol{z})$ and $\mathbb V(\boldsymbol{z})$, but we use reparametrization trick wich is to replace the sampler with equation:

$$\boldsymbol z = \mathbb E(\boldsymbol 
z) + \epsilon \cdot \sqrt {\mathbb V(\boldsymbol
 z) } \tag{1}$$

(1) is called **the reparametrization trick** since we use addition and multiplication to get the gradients we need for backpropagation. 

After we replaced the sampler with reparametrization trick we got $\boldsymbol z$ and lastly we use the **decoder** to decode the original input, but this time we call it $\boldsymbol{\hat x}$. It should be as close as possible to $\boldsymbol{x}$.

![vae spaces](/images/2021/04/vae.png)

In our image latent space is 2-dimensional $d=2$, and input space is 3-dimensional $n=3$. The green bubble around the point $\boldsymbol z$ denotes the variance. 

The manifold is where the inputs live. When we decode values from the latent space we should be as close as possible to the manifold.

## Gaussian noise $\boldsymbol \epsilon$

Bayesian would ask now is there some prior distribution used in VAE to regularize the process?

We use simple multivariate Gaussian noise with zero mean vector and identity covariance matrix $\mathbb I_d$, so $\boldsymbol \epsilon \sim \mathcal N(\boldsymbol 0, \mathbb I_d)$. We then write the objective function for VAE:

$$
\ell(\boldsymbol{x}, \boldsymbol{ \hat{x}})=\ell_{\text {reconstruction }}+ \ell_{\mathrm{KL}}\left(\boldsymbol{z}, \boldsymbol \epsilon \right) \tag{2}
$$

or in case we provide hyperparameter $\beta$ to control the relative entropy term:

$$
\ell(\boldsymbol{x}, \boldsymbol{ \hat{x}})=\ell_{\text {reconstruction }}+\beta \ell_{\mathrm{KL}}\left(\boldsymbol{z}, \boldsymbol \epsilon \right) \tag{3}
$$

Since for $\boldsymbol{z}$ we know mean vector and covariance matrix and we know same distribution details for for $\boldsymbol \epsilon$, we can calculate analytically the relative entropy formula for those multivariate Gaussian distributions:

$$
\beta \ell_{\mathrm{KL}}\left(\boldsymbol{z}, \boldsymbol \epsilon \right) = \frac{\beta}{2} \sum_{i=1}^{d}\left(\mathbb{V}\left(z_{i}\right)-\log \left[\mathbb{V}\left(z_{i}\right)\right]-1+\mathbb{E}\left(z_{i}\right)^{2}\right) \tag{4}
$$

The sum is due to the fact, that covariance matrix of $\boldsymbol z$ is actually a diagonal matrix (there are no covariances) so we can create a sum of $d$ independent dimensions to calculate relative entropy.

From the references you may find out that since we introduced variance of the latent variable we deal with $d$-dimensional bubbles. Loss function one can see trough three meaningful bubble forces:

* $\ell_{\text {reconstruction }}$ force to push bubbles away from each other
* $\mathbb{E}(z_{i})$ force to wrap all bubbles into a single big bubble
* $\mathbb{V}\left(z_{i}\right)-\log \left[\mathbb{V}\left(z_{i}\right)\right]-1$ force prevents bubbles to collapse or explode, keeping bubbles at unit size




---

**References**:

* [VAE paper](https://arxiv.org/pdf/1906.02691.pdf){:rel="nofollow"}
* [Yann LeCun VAE lectures from his course](https://atcold.github.io/pytorch-Deep-Learning/en/week08/08-3/){:rel="nofollow"}
* [Relative entropy formula](https://stats.stackexchange.com/a/60699/228453){:rel="nofollow"}
