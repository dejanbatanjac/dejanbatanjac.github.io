---
published: false
layout: post
title: Variational Auto-Encoder
permalink: /vae
---

There are two approaches to *inference*:
* exact inference
* approximation inference

**Exact_ inference** algorithms would be:

* Brute force
* The elimination algorithm
* Message passing
* Junction tree algorithm

**Approximation inference** algorithms wold be:
* Loopy belief propagation
* Variational inference
* Stochastic simulation 

Variational Auto-Encoder do **Variational Inference**.



<!-- ## Marginal Likelihood

To perform **inference** it is sufficient to reason in terms of probabilistic model **marginal likelihood** which is marginalization of **any missing** or latent variables in the model.

This integration is typically **intractable**, and instead, we optimize a lower bound on the marginal likelihood.

VAE uses VI (Variational Inference)

VI is algorithm to compute the posterior probability **approximately** and where **neural net** is learned to do so.


## Probabilistic machine learning

A probabilistic model is a joint distribution of hidden variables $\mathrm{z}$ and observed variables $\mathrm{x}$

$$
p(\mathbf{z}, \mathbf{x})
$$
Inference about the unknowns is through the posterior, the conditional distribution of the hidden variables given the observations
$$
p(\mathbf{z} \mid \mathbf{x})=\frac{p(\mathbf{z}, \mathbf{x})}{p(\mathrm{x})}
$$

> What is the evidence?
It's $p(x)$


For most interesting models, the denominator is not tractable. We appeal to approximate posterior inference. -->

Variational Auto-Encoder assumes:


* input $x$
* latent space $z$
* VAE parameters $\theta$
* PDF of joint $P_{\theta}(x,z)$

We assume the PDF is differentiable.

The whole idea of VAE is to create it as a neural net that approximates the **posterior**.

Posterior distribution we usually write with the letter Q so it will be $Q(z \mid x)$. This is a distribution of latent variable condition the data. This distribution is **continuous**, not the discrete point estimation as we will see.

Since we will learn the VAE as neural net, all the neural net parameters are weights so we can rewrite the distribution with respect to the weights: $Q_{w}(z \mid x)$.

We ask what is the marginal likelihood $P(\theta)$, and what is the variational lover bound of it?

$log P_{\theta}(x)= D_{KL}(Q_{z \mid x} \parallel P_{z \mid x}) +\mathcal{L}(\theta, w \mid x)$

where:

$\mathcal{L}({\theta}, w \mid x)
 = \mathbb{E}_{Q_{w}(z \mid x)}[\log P_{\theta}(x, z)-\log Q_{w}(z \mid x)]$



## Maximum likelihood

Maximum likelihood is a method for estimating parameters by maximizing the probability of the observed data. The main ingredients are:

 - The data: $D=(X,Y)$
 - The model parameters: $\theta$
 - The model that relates data to the parameters: $P(D|\theta)$ (which can be written differently, depending on the situation: $P(Y|X, \theta)$ is one of the possibilities.)

One would typically name the probability of observing data given parameters *likelihood* and write it as $L(\theta|D)=P(D|\theta)$. This is just a change of notation. One then maximizes the likelihood in respect to the values of the parameters:
\begin{equation}
\hat{\theta}=\mathrm{argmax}_\theta L(\theta|D).
\end{equation}

Quite often the likelihood is a product of many identical functions for different data points. In this case it is mathematically more convenient to maximize its logarithm, i.e. the *log-likelihood*: $LL(\theta|D) = \log L(\theta|D)$. The likelihood and its logarithm has the same maximum, since logarithm is a monotonous function. Finally, maximizing a function is the same as minimizing its negative, therefore minimizing a *negative log-likelihood* is the same as maximizing the *log-likelihood*, which is the same as maximizing the likelihood. This is important since the optimization algorithms are often spelled explicitly only for minimization (or only for maximization).

When d is dimension of multivariate normal then the PDF is given as:

p(x) = $(2 \pi)^{-\frac{d}{2}} \operatorname{det}(\boldsymbol{\Sigma})^{-\frac{1}{2}} e^{-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^{\top} \boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})}$

Where $\Sigma$ is positive semi definite.




A maximum likelihood estimator coincides with the most probable Bayesian estimator given a uniform prior distribution on the parameters. Indeed, the maximum a posteriori estimate is the parameter $\theta$ that maximizes the probability of $\theta$ given the data, given by
Bayes' theorem:
$$
\mathbb{P}\left(\theta \mid x_{1}, x_{2}, \ldots, x_{n}\right)=\frac{f\left(x_{1}, x_{2}, \ldots, x_{n} \mid \theta\right) \mathbb{P}(\theta)}{\mathbb{P}\left(x_{1}, x_{2}, \ldots, x_{n}\right)}
$$
where $\mathbb{P}(\theta)$ is the prior distribution for the parameter $\theta$ and where $\mathbb{P}\left(x_{1}, x_{2}, \ldots, x_{n}\right)$ is the probability of the data averaged over all parameters. Since the denominator is independent of $\theta,$ the Bayesian estimator is obtained by maximizing $f\left(x_{1}, x_{2}, \ldots, x_{n} \mid \theta\right) \mathbb{P}(\theta)$ with respect to $\theta$. If we further assume that the prior $\mathbb{P}(\theta)$ is a uniform distribution, the Bayesian estimator is obtained by maximizing the likelihood function $f\left(x_{1}, x_{2}, \ldots, x_{n} \mid \theta\right)$. Thus the Bayesian estimator coincides with the maximum likelihood estimator for a uniform prior distribution $\mathbb{P}(\theta)$.

https://math.stackexchange.com/questions/892832/why-we-consider-log-likelihood-instead-of-likelihood-in-gaussian-distribution/892874#892874


It means expectation with respect to $q_{\phi}\left(\mathbf{z} \mid \mathbf{x}^{(i)}\right) .$ So:
$$
\mathbb{E}_{q_{\phi}\left(\mathbf{z} \mid \mathbf{x}^{(i)}\right)}\left[\log p_{\theta}\left(\mathbf{x}^{(i)} \mid \mathbf{z}\right)\right]=\int_{\mathbb{R}^{d}} q_{\phi}\left(\mathbf{z} \mid \mathbf{x}^{(i)}\right) \log p_{\theta}\left(\mathbf{x}^{(i)} \mid \mathbf{z}\right) d \mathbf{z}
$$
Where without further information on the dimensionality of $\mathbf{z}$ I have assumed it to be in $\mathbb{R}^{d}$.
To further clarify, note that the underlying random vector/source of randomness is $\mathbf{z}$, of which you are computing the expectation of a function $f(\mathbf{z}),$ where $f(\mathbf{z})=\log p_{\theta}\left(\mathbf{x}^{(i)} \mid \mathbf{z}\right) .$ And this underlying source of randomness is captured in the distribution $q_{\phi}\left(\mathbf{z} \mid \mathbf{x}^{(i)}\right)$.


$\begin{aligned} \mathbb{E}_{q\left(\mathbf{z} \mid \mathbf{x}^{(i)} ; \phi\right)}\left[\log p_{\theta}\left(\mathbf{x}^{(i)} \mid \mathbf{z}\right)\right] &=\int_{\mathbb{R}^{d}} q\left(\mathbf{z} \mid \mathbf{x}^{(i)} ; \phi\right) \log p\left(\mathbf{x}^{(i)} \mid \mathbf{z} ; \theta\right) d \mathbf{z} \\ &=h(\phi, \theta) \end{aligned}$

## Notation for autoencoder

![VAE](/images/2021/04/vae.png)

## Joke 
> What is postarior for the VAE?
There is no posterior.


## Re-parametrization trick

https://youtu.be/7Rb4s9wNOmc?list=PLLHTzKZzVU9eaEyErdV26ikyolxOsz6mq&t=931