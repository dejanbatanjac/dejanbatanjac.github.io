---
published: true
layout: post
title: CNNs
---
CNNs invented by Yan LeCun in 1989 became popular for image recognition tasks.
 
Historically CNNs are great for both audio and image tasks.
 
 
## Image tasks
 
 
When dealing with image classification of big images CNNs are a much better solution than plain fully connected neural networks (FCNN) because CNNs are memory efficient compared to FCNN.
 
"Curse of dimensionality" is just a fancy name for the fact that images live in high dimensional vector spaces.
 
 
The [Imagenet challenge](https://image-net.org/challenges/LSVRC/){:rel="nofollow"} determined the historical evolution of CNNs starting from the [AlexNet model](https://en.wikipedia.org/wiki/AlexNet){:rel="nofollow"}.
 
 
The key feature traditional CNNs have is they are equivariant with respect to translation. This means if you have the cat on an image, the CNN will recognize the cat no matter where the cat is positioned if the cat is not rotated. This is an important feature that made CNNs practical for modern image tasks: 
image segmentation, object detection, image captioning, object classification, ... no matter where the object is located in the image.
 
> You can construct special CNNs that are equivariant wrt. rotation. These CNNs can recognize rotated objects.
 
> Instead of term equivariant some sources also use the term invariant.
 
CNNs have filters. These filters are what CNNs learn. Two important things:
 
* multiple filters for each convolution layer
* filters are shared across neurons
 
 
 
## Mimicking the human brain
 
"Curse of dimensionality" is just a fancy name for the fact that images live in high dimensional vector spaces.
 
CNNs were able to find the solution for this *curse* by mimicking the human brain. This was evident after the Study of the Visual Cortex from Hubel and Wiesel (1964).
 
This work was so impactful that they won the Nobel prize in 1981.


## Activation maps


For the input image of volume 224x224x3 we would like to apply 64 filters of size 5x5. What will we get at the end of the convolution layer?

The volume we get is called **activation map**.

To calculate the size of the activation map we need to *convolve* with each filter of size 5x5 over all possible input cuts of size 5x5 after what we get the dot product. You can understand this dot product result as how the filter likes the specific area of the input image. The dot product may be positive, negative or zero.

For each specific dot product we call the [*receptive filed*](https://theaisummer.com/receptive-field/){:rel=nofollow} is the small area of the input image that formed this dot product together with the filters.

You will get the new activation map of size: `224-5+1=220px`. The result will be a bit different if we involve the padding of the original image. The padding means we put the pixels with 0 value to the image border. If we use 2px padding the original 224px image will become 228px image because the 2px addition will be added both sides.
Now the calculus changes a little bit. The new receptive field will become:
`224+2*2-5+1=224px`

There is another thing may happen called stride. Stride means we move our filter S pixes each time. In case stride=2 the calculus becomes of final form:

$W_o = (W_1+2*P-F_w)/S+1$

$H_o = (H_1+2*P-F_h)/S+1$



## The number of parameters of Convolution layer

To calculate the number of parameters for the conv layer on the input 224x224x3 where the filer is 5x5 and there are 64 filters we use the formula:

`3*5*5*64+64`

We add 64 because there are 64 filter biases and the rest just multiplication of the input and output volumes and the filter size.

### Should we use bias in conv2d?

It is possible to use bias, but often it is ignored, by setting it with `bias=False` in PyTorch. This is because we usually use the BN behind the conv layer which has bias itself.

Empirically in a large model, removing the bias inputs makes very little difference because each node can make a bias node out of the average activation of all of its inputs, which by the law of large numbers will be roughly normal. 

Should we use bias in conv2d
 
```
nn.Conv2d(1, 20, 5, bias=False)
```
## Why do we use pooling layers in CNNs?
 
One of the reasons to use poling layers is to decrease the receptive field.
 
Say we are having a 1000 by 1000 pixels image and we will just use 3x3 convolution with ~~~ . Then the receptive field will be 500 layers till we get to the end. With pooling layers we make them smaller ...
 
If there would be no pooling layer we would just use convolution after convolution
We use pooling
 
### Why do we have max pooling to lower the resolution and at the same time we increase the number of filters?
 
By increasing the number of filters and by lowering the image using max pooling we try to keep the same number of features.
 
<!-- ### Why at the very start of the conv model we have >3 convolution kernels? (5,7,11)?
 
XXX -->
 
### What nn.Conv2d(3,10, 2,2) numbers 3 and 10?
 
The `in_channels` in the beginning is `3` for images with 3 channels (colored images). For images black and white it should be 1. Some satellite images may have 4 in there.
 
The `out_channels` is the number of convolution filters we have: `10`. The filters will be of size 2x2.
 
### What is dilation?
 
To explain what dilation is you can simple understand from these two images:
 
![IMG](/images/conv2.png)
![IMG](/images/conv3.png)
 
### Why a 3x3 filter is the best.
 
According to the [paper](https://cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf) from Max Zeiler.
17.3.3346
 
### Few more tips about convolution
 
* Convolution is position invariant and handles location, but not actions.
* In PyTorch convolution is actually implemented as correlation.
* In PyTorch `nn.ConvNd` and `F.convNd` do have reverse order of parameters.
 
 
### Bag of tricks for CONV networks
 
This [Bag of tricks](http://openaccess.thecvf.com/content_CVPR_2019/papers/He_Bag_of_Tricks_for_Image_Classification_with_Convolutional_Neural_Networks_CVPR_2019_paper.pdf) paper presents many tricks to be used for Convolutional Neural Networks such as:
 
* Large batch training
* Low precision training
* Decay of the learning rate
* Resnet tweaks
* Label smoothing
* Mixup training
* Transfer learning
* Semantic segmentation