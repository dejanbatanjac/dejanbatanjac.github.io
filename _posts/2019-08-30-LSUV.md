---
published: false
layout: post
title: LSUV
---

LSUV procedure could be viewed as batch normalization of layer output done only before the start of training. It is based on [All you need is a good init](https://arxiv.org/abs/1511.06422) paper.

LSUV can be applied on any neural network architecture in two steps: (I) pre-initialize weights of each convolution or linear layer with orthonormal matrices, and (II) from the first to the last layer normalize the variance of the output of each layer to be equal to one and mean to zero.

Here presented is the initialization algorithm even without the orthonormal matrices initialization since it works even this way.

However or network has one interesting nonlinearity in it. The Mish function.
In PyTorch it is `x * torch.tanh(F.softplus(x))`. 




