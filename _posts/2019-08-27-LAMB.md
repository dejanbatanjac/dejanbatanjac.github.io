---
published: false
layout: post
title: LAMB
---

LAMB is another optimizer algorithm for the first time presented in this [paper](https://arxiv.org/abs/1904.00962).

They claimed to train BERT model for the 76 minutes, which is excellent knowing that the previous results took over a day on the same architecture.

This is achieved by increasing the batch size to the memory limit of a GPU (TPU)

Let's dig into it.

LAMB is based on Adam so I will shortly provide a reminder for Adam:

$ p_{t+1} = p_t - lr \frac {m_t} { \sqrt{v_t} + \epsilon} $,

where

$ m_t = \frac {\beta_1 m_{t-1}+ (1-\beta_1)grad_{t-1}} {1-\beta_1^t}$, 
$ v_t = \frac {\beta_2 v_{t-1}+ (1-\beta_2)grad_{t-1}^2} {1-\beta_2^t}$

and $lr$ is the learning rate,

$grad_t$ is gradient tensor, 

$grad_t^2$ is Hadamar product of gradient tensor

$m_0$ and $v_0$ are 0,

$\beta_1,\beta_2$ are usually 0.9 and 0.99,

and $\epsilon$ is some small number `1e-3` for instance.

Similar like in Adam algorithm we are interested into the gradients $grad_t$ and gradients squared $grad_t^2$ and we calculate the average grad $m_t$ and average gradient squared $v_t$ thanks to the EWMA (exponentially weighted moving average) formula and debiasing.

In LAMB we also use the `pomero` function.

    def pomero(t):
        '''power 2, mean, square root = pomero'''
        return t.pow(2).mean().sqrt()

Let's explain what `pomero` do:

    t = torch.randn(22)
    q = t.pow(2)
    m = q.mean()
    r = m.sqrt()
    plt.scatter(range(0,22), t) #blue
    plt.scatter(range(0,22), q) #orange
    plt.scatter(range(0,1), m) #green
    plt.scatter(range(1,2), r) #red

    p = pomero(t)
    plt.scatter(range(2,3), p) #violet

![IMG](/images/lamb1.png)

Blue dot's are data from the normal distribution, orange dots are squares of blues, green dot is the mean of oranges, and the red and violet dots are same `pomero`.

We created and named the function `pomero` to express the LAMB better. Don't forget that at the end LAMB is just a function how we update the parameters.

We need to calculate two "`pomeros`" `p1` and `p2`.

    pom1 = pomero(p.data)
    pom2 = pomero(step)

Where `p.data` is the parameter, and the `step` is a LAMB step.

Then the final update of param `p` would be 

    p.data = p.data - lr*pom1/pom2 * step

Or to express this in simple words we take the old param and add the gradient descent. In here `lr*pom1/pom2` is constant value for each bach and step is a tensor with the same dimension as the first order gradient.

Again `step` is debiased gradient momentum in nominator, and in denominator it is square root of the debiased gradient squared momentum.

Here is the simplified LAMB code in PyTorch.





















