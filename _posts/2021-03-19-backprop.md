---
published: false
layout: post
title: Backpropagation tricks
permalink: /backprop
---

Neural network may consist of:
* stacked layers
* dynamically connected circular layers 

Either case backprop works.

Backpropagation has been invented in the paper:

---

One of the key tricks is to keep the variance of the outputs about the same for all the layers.

If layer variances are not about the same:

* some layers may learn faster than others
* come layers may converge others may diverge

## Initialization

Layer initialization should also be such that input and output variances are about the same.

Layer initialization should be random.


## Regularization

Regularization is any technique that helps against the overfitting. In here just the $\mathcal L_1$ and $\mathcal L_2$ will be considered.

$\mathcal L_p$ objective functions are often considered as purely regressive losses that should not be used in classification, however this is not the case.

If may be shown that regression is like classification except the response variable is continuous. Continuous response can be replaced with discrete after replacing continuous distribution output with binomial or multinational distribution.


## Lasso or $L_{1}$ regularization

We usually call all the parameters as generic $\theta$, so regularizer function is $R(\theta)$ over all the parameters of interest. $L_{1}$ or $L^{1}$ loss is sum of absolute values of the individual parameters.

$\quad R(\theta)=\| \boldsymbol{w} \|_{1} =\sum_{i}\left|w_{i}\right|$

$\quad w_{i} \leftarrow w_{i}-\eta\left(\frac{\partial C}{\partial w_{i}}+\alpha \operatorname{sign} \left(w_{i}\right)\right)$

Weighs $w_i$ will shrink towards 0 if inputs present as parameters for $C$ cost function are not important.

## Weight Decay or $L_{2}$ regularization

We can express loss in terms of the cost function $C(.)$ and regularization term on weights where typically we use some small constant (impact on the loss).

$$\mathcal L=C(.) +\alpha R(w)$$ 

If this regularization term can be expressed as: $R(w) =\|w\|^{2}_2$ as norm we can show :

$\frac{\partial R}{\partial w_{i}}=2 w_{i}$

$w_{i} \leftarrow w_{i}-\eta \frac{\partial L}{\partial w_{i}}$

$w_{i} \leftarrow w_{i}-\eta \left(\frac{\partial C}{\partial w_{i}}+2 \alpha w_{i}\right)$

$w_{i} \leftarrow w_{i}(1-2 \eta \alpha)-\eta \frac{\partial C}{\partial w_{i}}$

Meaning when gradient of the cost function is none the weights will start exponentially to **decrease**, which should be seen as a form of regularization.

The L2 loss works under the assumption of Gaussian noise, which is not valid in general case, meaning noise is independent of local characteristics of the data e.g. image.

The MSE (mean squared error) or L2 loss is currently dominant error measure across very diverse fields including regression, pattern recognition, signal and image processing.

Reason why it is popular may be identified also. It is convex and differentiable. L2 provides MLE in case of i.i.d. Gaussian noise.

## Targetprop vs. backprop

...

## Gradients we use

g of $\mathcal L$ w.r.t. $w$.


## Whitening transformation


Image whitening is removing the easy part of the problem.

Take away the mean and decorate the data. What is left.





A **whitening transformation** or **sphering transformation** is a linear transformation that transforms a vector of random variables with a known covariance matrix into a set of new variables whose covariance is the identity matrix, meaning that they are uncorrelated and each have variance 1.

The transformation is called "whitening" because it changes the input vector into a white noise vector.