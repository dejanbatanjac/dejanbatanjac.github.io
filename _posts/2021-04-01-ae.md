---
published: false
layout: post
title: Auto-Encoder
permalink: /ae
---

Here is a typical notation for an autoencoder.

![autoencoder](/images/2021/04/ae.png)

This would be the input $\mathbb x \in \mathbb R^n$, the latent state $\mathbb z \in \mathbb R^d$, where $d$ is the dimension of the latent space, which may be $d \lt n$, but it may also be $d \ge n$.

Finally the $\mathbb {\hat x} \in \mathbb R^n$, again the same dimension of the input. 

We can write two equations here:

$$z=f(W_e x + b_e) \\ \hat x=f(W_d z + b_d)$$

where $f$ is some nonlinearity (activation function).
The dimensions of the matrices:

* $W_e \in \mathbb R^{d \times n}$ is the encoder matrix
* $W_d \in \mathbb R^{n \times d}$ is the decoder matrix


The very special case for the autoencoder is $W_e \dot = W_d^T,$ when the matrices are transposed.

## Loss functions

Where $m$ is the number of samples, and $\ell(x^{(j)}, \hat x^{(j)} )$ is _per sample loss_ we can express the loss as an **average** of _per sample losses_:

$$\mathcal{L}=\frac{1}{m} \sum_{j=1}^{m} \ell\left(\mathbb{x}^{(j)}, \hat{x}^{(j)}\right)$$

There are two frequent cases:

* when $x_i \in \{0,1\}$ we can use binary cross entropy loss where the outputs $\hat x$ will be in between 0 and 1 thanks to sigmoid function:
 
$$\quad \ell(\mathbb{x}, \hat{x})=-\sum_{i=1}^{n}\left[x_{i} \log \left(\hat{x}_{i}\right)+\left(1-x_{i}\right) \log \left(1-\hat{x}_{i}\right)\right]$$

* when $x_i \in \mathbb R$ we can use MSE for instance:

$$\quad \ell(\mathbb{x}, \hat{x})=\frac{1}{2}\|\mathbb{x}-\hat{x}\|^{2}$$

## The idea of the manifold

The idea of manifold is based on observation that  images of *something*, e.g. _facial expressions_ define a smooth manifold in the high dimensional image space (space of all the images).

If we somehow find this manifold we could move trough it and we would experience all the facial expressions.

This manifold is then highly connected with the facial degrees of freedom, head movement and head muscles movements.

Here is a more [complex defintion of manifold](https://en.wikipedia.org/wiki/Manifold){:rel="nofollow"}.

## Types of autoencoders

* Denoising autoencoder
* Sparse Autoencoder
* Deep Autoencoder
* Contractive Autoencoder
* Undercomplete Autoencoder
* Convolutional Autoencoder
* Variational Autoencoder

## Denoising autoencoder

![denois autoencoder](/images/2021/04/denoising-ae.png)

_In here $x$ is an image without the noise and $\tilde x$ image with noise._

Typically denoising autoencoders are trained on a specific image set say faces, and once trained we experiment with noisy image faces, and they should be denoised.


## Variational autoencoder

This type is possible the new GAN, and it will be explained in a separate [VAE article](/vae).


