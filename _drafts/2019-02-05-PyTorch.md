---
published: false
---
## PyTorch from tabula rasa

PyTorch is based on Torch. Torch is a Tensor library like Numpy. 
Unlike Numpy, Torch has strong GPU support. 
    
You can use Torch either using the Lua programming language or if you like Python you can use PyTorch.

You can use together all major Python packages likes scipy, numpy, matplotlib and Cython with PyTorch's own autograd system.

What is PyTorch autograd?

The autograd package provides automatic differentiation for all operations on Tensors. This is needed for the backpropagation algorithm to work. 

### Main PyTorch features
* Eager execution
* C++ support
* Native ONNX Support (open nn exchange)
* Supported on major cloud platforms
* Supports all major network model architectures

### Data types

|Data type | 	Tensor|
|---|:---|
|32-bit floating point |	torch.FloatTensor|
|64-bit floating point |	torch.DoubleTensor|
|16-bit floating point |	torch.HalfTensor|
|8-bit integer (unsigned)| 	torch.ByteTensor|
|8-bit integer (signed) | 	torch.CharTensor|
|16-bit integer (signed)| 	torch.ShortTensor|
|32-bit integer (signed)| 	torch.IntTensor |
64-bit integer (signed) |	torch.LongTensor

### Basic Algebra in PyTorch

Here we define a tensors `a` and `b` and do basic algebra operations:

~~~
a = torch.rand(4,4)
print(a)
b = torch.rand(4)
print(b)

mm = a.mm(a)       # matrix multiplication
mv = a.mv(b)       # matrix-vector multiplication
t= a.t()           # matrix transpose

print(mm)
print(mv)
print(t)
~~~
The output will be like this:

~~~
tensor([[0.9216, 0.0812, 0.3326, 0.1223],
        [0.4754, 0.1876, 0.8705, 0.0348],
        [0.8547, 0.2323, 0.1879, 0.7196],
        [0.0683, 0.3799, 0.8813, 0.9155]])
tensor([0.3089, 0.8028, 0.2625, 0.4179])
tensor([[1.1806, 0.2138, 0.5475, 0.4668],
        [1.2737, 0.2893, 0.5156, 0.7229],
        [1.1079, 0.4301, 1.1560, 0.9066],
        [1.0594, 0.6294, 1.3258, 1.4938]])
tensor([0.4883, 0.5404, 0.8006, 0.9401])
tensor([[0.9216, 0.4754, 0.8547, 0.0683],
        [0.0812, 0.1876, 0.2323, 0.3799],
        [0.3326, 0.8705, 0.1879, 0.8813],
        [0.1223, 0.0348, 0.7196, 0.9155]])
~~~

Also we have Hadamard product with * and the dot product

~~~
a = torch.Tensor([[1,2],[3,4]])
b = torch.Tensor([5,6])
c = torch.Tensor([7,8])

print(a*b) # element-wise matrix multiplication (Hadamard product)
print(a*a)
print(torch.dot(b,c)) # not like in numpy, works just on 1D Tensors
~~~
Result
~~~
tensor([[ 5., 12.],
        [15., 24.]])
tensor([[ 1.,  4.],
        [ 9., 16.]])
tensor(83.)
~~~

### Sequence models

### 